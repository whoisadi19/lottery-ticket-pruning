# Lottery Ticket Hypothesis - Neural Network Pruning

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Author:** Aditya Chowdhury  
**Institution:** NMIMS MPSTME Shirpur  
**Course:** Computer Science  
**Date:** January 2026

## ğŸ¯ Project Overview

This project implements and validates MIT's **Lottery Ticket Hypothesis**, demonstrating that neural networks can be pruned by **90% while maintaining (and even improving!) accuracy**. 

### Key Finding
- **90% pruning** â†’ 97.94% accuracy (improvement over baseline!)
- **80% pruning** â†’ 98.29% accuracy (best performance!)
- **10Ã— smaller model** with better performance

## ğŸ“„ Research Article

This implementation is part of a research article submitted to **NMIMS Tech Trends** technical magazine. The complete article explores:
- The Lottery Ticket Hypothesis theory
- Practical implementation details
- Experimental results and analysis
- Real-world implications for AI efficiency

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11 or higher
- pip package manager

### Installation

```bash
# Clone the repository
git clone https://github.com/whoisadi19/lottery-ticket-pruning.git
cd lottery-ticket-pruning

# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
.\venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Running the Experiment

**Option 1: Direct execution**
```bash
# Run the pruning experiment
python lottery_ticket_pruning.py

# Generate visualizations
python visualize_results.py
```

**Option 2: Using helper script (suppresses warnings)**
```bash
# Cleaner output without numpy warnings
python run_experiment.py
```

**Option 3: Test your setup first**
```bash
# Verify PyTorch installation
python test_setup.py
```

**Expected Runtime:** 15-20 minutes on CPU

## ğŸ“Š Results

### Experimental Setup
- **Dataset:** MNIST (70,000 handwritten digits)
- **Architecture:** 784 â†’ 300 â†’ 100 â†’ 10 (fully connected)
- **Training:** 5 epochs per model
- **Pruning Levels:** 0%, 20%, 40%, 60%, 70%, 80%, 90%, 95%

### Key Results

| Pruning % | Parameters | Accuracy | Change |
|-----------|-----------|----------|--------|
| Baseline  | 266,610   | 97.48%   | -      |
| 90%       | 26,617    | 97.94%   | +0.46% |
| 80%       | 53,237    | 98.29%   | +0.81% |

**Surprising Discovery:** Pruning actually *improves* accuracy by removing redundant parameters that add noise!

## ğŸ“ Project Structure

```
lottery-ticket-pruning/
â”œâ”€â”€ lottery_ticket_pruning.py   # Main implementation
â”œâ”€â”€ visualize_results.py        # Visualization generation
â”œâ”€â”€ run_experiment.py           # Helper script with warnings suppressed
â”œâ”€â”€ test_setup.py               # Quick setup verification
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ results/
â”‚   â””â”€â”€ experiment_results.json # Experimental data
â””â”€â”€ images/
    â”œâ”€â”€ accuracy_vs_pruning.png
    â”œâ”€â”€ parameter_reduction.png
    â”œâ”€â”€ accuracy_difference.png
    â”œâ”€â”€ combined_metrics.png
    â””â”€â”€ results_table.png
```

## ğŸ”¬ Implementation Details

### The Lottery Ticket Algorithm

1. **Initialize:** Create a neural network with random weights (save initial values!)
2. **Train:** Train the full network to convergence
3. **Prune:** Remove weights with smallest magnitudes
4. **Reset:** Reset remaining weights to their original initialization
5. **Retrain:** Train the sparse network from the original initialization

### Key Features

- âœ… Magnitude-based pruning
- âœ… Proper initialization handling
- âœ… Mask maintenance during training
- âœ… Comprehensive logging
- âœ… Result visualization

## ğŸ“ˆ Visualizations

The project generates 5 professional visualizations:

1. **Accuracy vs Pruning** - Shows accuracy remains high across pruning levels
2. **Parameter Reduction** - Dramatic visualization of model compression
3. **Accuracy Difference** - Highlights performance improvements
4. **Combined Metrics** - Dual-axis view of accuracy and model size
5. **Results Table** - Professional summary of all results

## ğŸ“ Educational Value

This project demonstrates:
- Deep learning fundamentals
- Neural network pruning techniques
- PyTorch implementation skills
- Experimental methodology
- Data visualization
- Scientific writing

## ğŸ“š References

1. Frankle, J., & Carbin, M. (2019). "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks." *ICLR 2019*.

2. Han, S., Pool, J., Tran, J., & Dally, W. (2015). "Learning both Weights and Connections for Efficient Neural Network." *NeurIPS 2015*.

## ğŸ¤ Contributing

This is a research project for academic submission. Feel free to:
- Open issues for questions or discussions
- Fork the repository for your own experiments
- Cite this work in your research

## ğŸ“§ Contact

**Aditya Chowdhury**
- Email: faithfullyours.adi@gmail.com
- GitHub: [@whoisadi19](https://github.com/whoisadi19)
- Institution: NMIMS MPSTME Shirpur

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- MIT researchers Jonathan Frankle and Michael Carbin for the original Lottery Ticket Hypothesis
- NMIMS MPSTME Shirpur Department of Computer Science
- NMIMS Tech Trends magazine

## â­ Star This Repository

If you find this project helpful or interesting, please consider giving it a star! It helps others discover this work.

---

**Made with â¤ï¸ for advancing AI efficiency research**
